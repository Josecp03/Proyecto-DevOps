# üß± Paso 3 - Terraform (Infraestructura como c√≥digo)

Terraform es una herramienta de **infraestructura como c√≥digo (IaC)** que nos permite definir, desplegar y gestionar recursos en la nube de forma autom√°tica y repetible mediante archivos de configuraci√≥n. En este caso, la vamos a usar para crear toda la infraestructura como redes, m√°quinas virtuales, cl√∫ster AKS. Todo esto se alojar√° en **Azure**, aprovechando los recursos que ya hemos creado previamente (Storage Account, contenedor `tfstate` y ACR), lo que permite que Terraform gestione la infraestructura de manera segura, modular y consistente. Por eso este paso se incluye aqu√≠: para preparar y desplegar autom√°ticamente todo lo necesario en Azure antes de pasar a los siguientes pasos del proyecto.

## üìã Tabla de contenidos

- [üß± Paso 3 - Terraform (Infraestructura como c√≥digo)](#-paso-3---terraform-infraestructura-como-c√≥digo)
  - [üìã Tabla de contenidos](#-tabla-de-contenidos)
  - [üõ†Ô∏è 3.1 Instalaci√≥n Terraform](#Ô∏è-31-instalaci√≥n-terraform)
  - [üó∫Ô∏è 3.2 Esquema de infraestructura](#Ô∏è-32-esquema-de-infraestructura)
    - [üß© Resource Group](#-resource-group)
    - [üåê Redes Virtuales (VNets)](#-redes-virtuales-vnets)
    - [üñ•Ô∏è Subnet DB (VNet-Compute)](#Ô∏è-subnet-db-vnet-compute)
    - [üß± Subnet AKS (VNet-AKS)](#-subnet-aks-vnet-aks)
    - [üì¶ Azure Container Registry (ACR)](#-azure-container-registry-acr)
    - [‚òÅÔ∏è Azure Storage Account](#Ô∏è-azure-storage-account)
    - [‚úÖ Puntos clave del dise√±o](#-puntos-clave-del-dise√±o)
  - [üìÇ 3.3 Estructura de archivos](#-33-estructura-de-archivos)
  - [üóÇÔ∏è 3.4 Archivos terraform](#Ô∏è-34-archivos-terraform)
    - [üöÄ 3.4.1 Archivos ra√≠z](#-341-archivos-ra√≠z)
      - [`backend.tf`](#backendtf)
      - [`variables.tf`](#variablestf)
      - [`terraform.tfvars`](#terraformtfvars)
      - [`main.tf`](#maintf)
      - [`outputs.tf`](#outputstf)
    - [üåê 3.4.2 M√≥dulo networking](#-342-m√≥dulo-networking)
      - [`modules/networking/main.tf`](#modulesnetworkingmaintf)
      - [`modules/networking/variables.tf`](#modulesnetworkingvariablestf)
      - [`modules/networking/outputs.tf`](#modulesnetworkingoutputstf)
    - [üñ•Ô∏è 3.4.3 M√≥dulo compute](#Ô∏è-343-m√≥dulo-compute)
      - [`modules/compute/main.tf`](#modulescomputemaintf)
      - [`modules/compute/variables.tf`](#modulescomputevariablestf)
      - [`modules/compute/outputs.tf`](#modulescomputeoutputstf)
    - [üîó 3.4.4 M√≥dulo peering](#-344-m√≥dulo-peering)
      - [`modules/peering/main.tf`](#modulespeeringmaintf)
      - [`modules/peering/variables.tf`](#modulespeeringvariablestf)
      - [`modules/peering/outputs.tf`](#modulespeeringoutputstf)
    - [‚ò∏Ô∏è 3.4.5 M√≥dulo aks](#Ô∏è-345-m√≥dulo-aks)
      - [`modules/aks/main.tf`](#modulesaksmaintf)
      - [`modules/aks/variables.tf`](#modulesaksvariablestf)
      - [`modules/aks/outputs.tf`](#modulesaksoutputstf)
  - [üöÄ 3.5 Desplegar la infraestructura](#-35-desplegar-la-infraestructura)

---

## üõ†Ô∏è 3.1 Instalaci√≥n Terraform

* Para instalar la versi√≥n m√°s reciente de terraform debemos descargar el binario de Terraform desde HashiCorp:

    ```bash
    wget https://releases.hashicorp.com/terraform/1.13.1/terraform_1.13.1_linux_amd64.zip
    ```

* Instalar unzip si no lo tenemos ya instalado:

    ```bash
    sudo apt install unzip
    ```

* Descomprimir el archivo:

    ```bash
    unzip terraform_1.13.1_linux_amd64.zip
    ```

* Mover el binario a /usr/local/bin para que est√© disponible en todo el sistema:

    ```bash     
    sudo mv terraform /usr/local/bin/
    ```

* Y comprobar que se ha instalado correctamente:

    ```bash
    terraform --version
    ```

[üîù Volver a la tabla de contenidos üîù](#-tabla-de-contenidos) 

---

## üó∫Ô∏è 3.2 Esquema de infraestructura

Para definir la infraestructura de este proyecto me apoy√© en la herramienta [BrainBoard](https://braiboard.com/), lo que me permiti√≥ organizar visualmente los distintos componentes de la soluci√≥n y c√≥mo se relacionan entre s√≠. A partir de esa planificaci√≥n elabor√© un esquema propio que refleja con claridad la arquitectura que se va a desplegar en Azure.

En el dise√±o se pueden observar los principales elementos: dos redes virtuales interconectadas mediante peering, las m√°quinas virtuales para base de datos y backup, el cl√∫ster de Kubernetes (AKS), el balanceador de carga, el registro de contenedores (ACR) y la cuenta de almacenamiento. Todo ello se encuentra centralizado bajo un √∫nico grupo de recursos, lo que facilita la gesti√≥n y la trazabilidad del entorno.

![](imgs/03/1.png)

A continuaci√≥n, se detallan los distintos bloques que conforman la infraestructura:

### üß© Resource Group

Toda la infraestructura se encuentra dentro de un √∫nico **Resource Group de Azure**, lo que simplifica la administraci√≥n, el despliegue y, en caso necesario, la eliminaci√≥n de todos los recursos de forma ordenada.

### üåê Redes Virtuales (VNets)

La arquitectura utiliza **dos VNets separadas** para organizar mejor las cargas de trabajo:

- **VNet-Compute (10.0.0.0/16):** contiene las m√°quinas virtuales de base de datos y backup.  
- **VNet-AKS (10.1.0.0/16):** reservada exclusivamente para el cl√∫ster de Kubernetes.  

Ambas VNets est√°n interconectadas mediante **VNet Peering**, lo que permite la comunicaci√≥n privada y segura entre ellas sin exponer recursos sensibles a Internet.

### üñ•Ô∏è Subnet DB (VNet-Compute)
  
Dentro de la **VNet-Compute** se encuentra la **subnet DB (10.0.2.0/24)**, que incluye:

- **VM Database (PostgreSQL):** aloja la base de datos del sistema. Solo tiene IP privada, de modo que √∫nicamente puede ser accedida desde la red interna.  
  
- **VM Backup / Disaster Recovery:** act√∫a como jump server. Tiene IP p√∫blica para acceso remoto por SSH y IP privada para comunicarse con la base de datos.

**Seguridad (NSG)**

- **NSG-DB:** solo permite conexiones internas (SSH y PostgreSQL desde 10.0.2.0/24).  
- **NSG-BKP:** permite SSH (22) desde cualquier origen, ya que es el punto de acceso externo a la infraestructura.  

De este modo, toda la administraci√≥n remota pasa primero por la VM de backup y nunca directamente contra la base de datos.

### üß± Subnet AKS (VNet-AKS) 
  
En la **subnet AKS (10.1.1.0/24)** se despliega el cl√∫ster de **Azure Kubernetes Service (AKS)**, donde se ejecutan los servicios de la aplicaci√≥n:  

- **Pods frontend:** 2 r√©plicas.  
- **Pods backend:** 2 r√©plicas.  

El tr√°fico externo llega al cl√∫ster a trav√©s de un **Load Balancer con IP p√∫blica**, que distribuye las peticiones entre los pods.  
El **VNet Peering** garantiza que AKS puede acceder directamente a la base de datos sin necesidad de exponerla p√∫blicamente.

> El Load Balancer es un servicio que reparte autom√°ticamente el tr√°fico entre varios recursos (en este caso, los pods de AKS) para mejorar disponibilidad y rendimiento; aqu√≠ tiene IP p√∫blica para recibir peticiones externas sin exponer la base de datos.

### üì¶ Azure Container Registry (ACR)
  
El **ACR** se utiliza para almacenar y distribuir las im√°genes Docker. 

- Las im√°genes se construyen con GitHub Actions (Tambi√©n se pueden construir y publicar desde la terminal en local) y se publican en el registro.  
- Posteriormente, AKS descarga y ejecuta dichas im√°genes en los pods.  

Este desacoplamiento asegura un flujo CI/CD seguro y eficiente.

> El flujo CI/CD automatiza la construcci√≥n, prueba y despliegue de la aplicaci√≥n, asegurando que los cambios lleguen de forma controlada y r√°pida a los entornos de ejecuci√≥n; m√°s adelante profundizaremos en c√≥mo se implementa concretamente en este proyecto.

### ‚òÅÔ∏è Azure Storage Account
  
El **Storage Account** cumple un papel cr√≠tico en la infraestructura:  

- Contenedor `tfstate`: almacena el archivo de estado de Terraform, necesario para gestionar la infraestructura como c√≥digo de manera consistente y colaborativa.
- Contenedor `backups`: guarda copias de seguridad de la base de datos y de las VMs. En caso de fallo o desastre, estas copias pueden restaurarse de forma r√°pida, garantizando continuidad de servicio. 

De esta forma, el almacenamiento asegura persistencia, trazabilidad y recuperaci√≥n ante desastres, centralizando tanto la configuraci√≥n de la infraestructura como los datos cr√≠ticos del sistema.

### ‚úÖ Puntos clave del dise√±o
  
- Solo la **VM de backup** y el **Load Balancer de AKS** tienen IP p√∫blica.  
- Todo lo dem√°s queda en red privada, protegido por las VNets y el peering.  
- La seguridad de acceso est√° reforzada con NSGs y el uso de un **jump server** (VM backup). 

---

## üìÇ 3.3 Estructura de archivos

En este apartado se presenta la estructura de los archivos de Terraform utilizada en el proyecto, mostrando c√≥mo se han organizado los distintos ficheros y carpetas para gestionar de forma ordenada y modular la infraestructura de Azure.

```
Deploy-DevOps-App/
‚îî‚îÄ‚îÄ iac/
    ‚îú‚îÄ‚îÄ backend.tf                      ‚Üê Configuraci√≥n del backend remoto (Azure Storage)
    ‚îú‚îÄ‚îÄ main.tf                         ‚Üê Llamadas a m√≥dulos (networking, compute, aks, peering)
    ‚îú‚îÄ‚îÄ outputs.tf                      ‚Üê Salidas (outputs) finales de Terraform
    ‚îú‚îÄ‚îÄ variables.tf                    ‚Üê Definici√≥n de variables globales
    ‚îú‚îÄ‚îÄ terraform.tfvars                ‚Üê Valores por defecto (Oculto en GitHub)
    ‚îî‚îÄ‚îÄ modules/                        ‚Üê Carpeta de m√≥dulos reutilizables
        ‚îú‚îÄ‚îÄ networking/
        ‚îÇ   ‚îú‚îÄ‚îÄ main.tf                 ‚Üê Crea VNet + subnets 
        ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf            ‚Üê Variables espec√≠ficas de networking
        ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf              ‚Üê Salidas: vnet_compute_id, vnet_aks_id, subnet_db_id, subnet_aks_id
        ‚îú‚îÄ‚îÄ compute/
        ‚îÇ   ‚îú‚îÄ‚îÄ main.tf                 ‚Üê Crea las dos VMs (DB y Backup) + NICs
        ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf            ‚Üê Variables espec√≠ficas de compute
        ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf              ‚Üê Salidas: vm_db_private_ip, vm_bkp_private_ip, vm_bkp_public_ip, db_nsg_id, bkp_nsg_id
        ‚îú‚îÄ‚îÄ peering/
        ‚îÇ   ‚îú‚îÄ‚îÄ main.tf                 ‚Üê Configura el VNet Peering entre VNets
        ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf            ‚Üê Variables del peering
        ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf              ‚Üê Salidas: peering_compute_to_aks_id, peering_aks_to_compute_id
        ‚îú‚îÄ‚îÄ aks/
        ‚îÇ   ‚îú‚îÄ‚îÄ main.tf                 ‚Üê Crea el cl√∫ster AKS 
        ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf            ‚Üê Variables espec√≠ficas de AKS
        ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf              ‚Üê Salidas: cluster_name, kube_config
        ‚îî‚îÄ‚îÄ
```

[üîù Volver a la tabla de contenidos üîù](#-tabla-de-contenidos) 

---

## üóÇÔ∏è 3.4 Archivos terraform

A continuaci√≥n se muestran **todos** los archivos **.tf** que forman la infraestructura de Terraform, respetando la jerarqu√≠a de carpetas y m√≥dulos.

### üöÄ 3.4.1 Archivos ra√≠z

#### `backend.tf` 

Define la versi√≥n m√≠nima de Terraform, el proveedor `azurerm` y el backend remoto en Azure Blob Storage.

```hcl
terraform {
  required_version = ">= 1.1.0"

  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
  }

  backend "azurerm" {
    resource_group_name  = "rg-proyecto-devops"
    storage_account_name = "stajosecp03devops"
    container_name       = "tfstate"
    key                  = "infrastructure.tfstate"
  }
}

provider "azurerm" {
  features {}
}
```

---

#### `variables.tf` 

Contiene las variables globales.

```hcl
# General
variable "location" { type = string }
variable "rg_name"  { type = string }

# Compute VNet
variable "vnet_compute_name" { type = string }
variable "vnet_compute_cidr" { type = string }
variable "subnet_db_name"    { type = string }
variable "subnet_db_cidr"    { type = string }

# AKS VNet
variable "vnet_aks_name"    { type = string }
variable "vnet_aks_cidr"    { type = string }
variable "subnet_aks_name"  { type = string }
variable "subnet_aks_cidr"  { type = string }

# VMs
variable "vm_db_name"       { type = string }
variable "vm_bkp_name"      { type = string }
variable "vm_admin_user"    { type = string }
variable "ssh_public_key"   { type = string }

# AKS
variable "aks_name"         { type = string }
variable "aks_node_count"   { type = number }
variable "aks_node_vm_size" { type = string }
```

---

#### `terraform.tfvars` 

Se definen valores por defecto que suelen aplicarse en la mayor√≠a de los casos.

```hcl
# General
location             = "westeurope"
rg_name              = "rg-proyecto-devops"

# VNet Compute
vnet_compute_name    = "vnet-compute"
vnet_compute_cidr    = "10.0.0.0/16"
subnet_db_name       = "subnet-db"
subnet_db_cidr       = "10.0.2.0/24"

# VNet AKS
vnet_aks_name        = "vnet-aks"
vnet_aks_cidr        = "10.1.0.0/16"
subnet_aks_name      = "subnet-aks"
subnet_aks_cidr      = "10.1.1.0/24"

# VMs
vm_db_name           = "vm-database"
vm_bkp_name          = "vm-backup"
vm_admin_user        = "josecp03"
ssh_public_key       = "ssh-rsa AAAAB3NzaC1y..."

# AKS
aks_name             = "aks-josecp03"
aks_node_count       = 2
aks_node_vm_size     = "Standard_B2s"
```

---

#### `main.tf` 

Este archivo hace de orquestador: lee el Resource Group, luego invoca a cada subm√≥dulo.

```hcl
data "azurerm_resource_group" "rg" {
  name = var.rg_name
}

module "networking" {
  source            = "./modules/networking"
  location          = var.location
  resource_group    = data.azurerm_resource_group.rg.name
  vnet_compute_name = var.vnet_compute_name
  vnet_compute_cidr = var.vnet_compute_cidr
  subnet_db_name    = var.subnet_db_name
  subnet_db_cidr    = var.subnet_db_cidr
  vnet_aks_name     = var.vnet_aks_name
  vnet_aks_cidr     = var.vnet_aks_cidr
  subnet_aks_name   = var.subnet_aks_name
  subnet_aks_cidr   = var.subnet_aks_cidr
}

module "peering" {
  source         = "./modules/peering"
  resource_group = data.azurerm_resource_group.rg.name
  vnet_compute_id = module.networking.vnet_compute_id
  vnet_aks_id     = module.networking.vnet_aks_id
}

module "compute" {
  source           = "./modules/compute"
  location         = var.location
  resource_group   = data.azurerm_resource_group.rg.name
  subnet_db_id     = module.networking.subnet_db_id
  vm_db_name       = var.vm_db_name
  vm_bkp_name      = var.vm_bkp_name
  admin_user       = var.vm_admin_user
  ssh_public_key   = var.ssh_public_key
}

module "aks" {
  source             = "./modules/aks"
  location           = var.location
  resource_group     = data.azurerm_resource_group.rg.name
  cluster_name       = var.aks_name
  node_count         = var.aks_node_count
  node_vm_size       = var.aks_node_vm_size
  subnet_aks_id      = module.networking.subnet_aks_id
}
```

---

#### `outputs.tf` 

Recolecta los valores ‚Äú√∫tiles‚Äù que quiero consultar al final, una vez terminado el `apply`.

```hcl
output "vm_db_private_ip" {
  value       = module.compute.vm_db_private_ip
  description = "IP privada de la VM de base de datos"
}

output "vm_bkp_public_ip" {
  value       = module.compute.vm_bkp_public_ip
  description = "IP p√∫blica de la VM de backup (jump server)"
}

output "aks_kubeconfig" {
  value       = module.aks.kube_config
  description = "Kubeconfig del cl√∫ster AKS"
  sensitive   = true
}
```

[üîù Volver a la tabla de contenidos üîù](#-tabla-de-contenidos) 

---

### üåê 3.4.2 M√≥dulo networking

#### `modules/networking/main.tf` 

```hcl
# VNet para Compute
resource "azurerm_virtual_network" "compute" {
  name                = var.vnet_compute_name
  address_space       = [var.vnet_compute_cidr]
  location            = var.location
  resource_group_name = var.resource_group
}

resource "azurerm_subnet" "db" {
  name                 = var.subnet_db_name
  resource_group_name  = var.resource_group
  virtual_network_name = azurerm_virtual_network.compute.name
  address_prefixes     = [var.subnet_db_cidr]
}

# VNet para AKS
resource "azurerm_virtual_network" "aks" {
  name                = var.vnet_aks_name
  address_space       = [var.vnet_aks_cidr]
  location            = var.location
  resource_group_name = var.resource_group
}

resource "azurerm_subnet" "aks" {
  name                 = var.subnet_aks_name
  resource_group_name  = var.resource_group
  virtual_network_name = azurerm_virtual_network.aks.name
  address_prefixes     = [var.subnet_aks_cidr]
}
```

---

#### `modules/networking/variables.tf` 

```hcl
variable "location"          { type = string }
variable "resource_group"    { type = string }

variable "vnet_compute_name" { type = string }
variable "vnet_compute_cidr" { type = string }
variable "subnet_db_name"    { type = string }
variable "subnet_db_cidr"    { type = string }

variable "vnet_aks_name"     { type = string }
variable "vnet_aks_cidr"     { type = string }
variable "subnet_aks_name"   { type = string }
variable "subnet_aks_cidr"   { type = string }
```

---

#### `modules/networking/outputs.tf` 

```hcl
output "vnet_compute_id" { value = azurerm_virtual_network.compute.id }
output "vnet_aks_id"     { value = azurerm_virtual_network.aks.id }

output "subnet_db_id"  { value = azurerm_subnet.db.id }
output "subnet_aks_id" { value = azurerm_subnet.aks.id }
```

[üîù Volver a la tabla de contenidos üîù](#-tabla-de-contenidos) 

---

### üñ•Ô∏è 3.4.3 M√≥dulo compute

#### `modules/compute/main.tf` 

```hcl
# NSG para VM Backup (acceso p√∫blico)
resource "azurerm_network_security_group" "bkp_nsg" {
  name                = "nsg-${var.vm_bkp_name}"
  location            = var.location
  resource_group_name = var.resource_group

  security_rule {
    name                       = "Allow-SSH-External"
    priority                   = 1000
    direction                  = "Inbound"
    access                     = "Allow"
    protocol                   = "Tcp"
    source_port_range          = "*"
    destination_port_range     = "*"
    source_address_prefix      = "*"
    destination_address_prefix = "*"
  }

  tags = {
    Environment = "Development"
    Purpose     = "Backup-VM-Access"
  }
}

# NSG para VM Database (solo interno)
resource "azurerm_network_security_group" "db_nsg" {
  name                = "nsg-${var.vm_db_name}"
  location            = var.location
  resource_group_name = var.resource_group

  security_rule {
    name                       = "Allow-SSH-Internal"
    priority                   = 1000
    direction                  = "Inbound"
    access                     = "Allow"
    protocol                   = "Tcp"
    source_port_range          = "*"
    destination_port_range     = "22"
    source_address_prefix      = "10.0.2.0/24"
    destination_address_prefix = "*"
  }

  security_rule {
    name                       = "Allow-MySQL"
    priority                   = 1001
    direction                  = "Inbound"
    access                     = "Allow"
    protocol                   = "Tcp"
    source_port_range          = "*"
    destination_port_range     = "3306"
    source_address_prefix      = "10.0.2.0/24"
    destination_address_prefix = "*"
  }

  security_rule {
    name                       = "Allow-PostgreSQL"
    priority                   = 1002
    direction                  = "Inbound"
    access                     = "Allow"
    protocol                   = "Tcp"
    source_port_range          = "*"
    destination_port_range     = "5432"
    source_address_prefix      = "10.0.2.0/24"
    destination_address_prefix = "*"
  }

  tags = {
    Environment = "Development"
    Purpose     = "Database-VM-Internal"
  }
}

# IP p√∫blica para VM Backup
resource "azurerm_public_ip" "bkp" {
  name                = "${var.vm_bkp_name}-pip"
  location            = var.location
  resource_group_name = var.resource_group
  allocation_method   = "Static"
  sku                 = "Standard"
}

# NIC Database (solo privada)
resource "azurerm_network_interface" "db" {
  name                = "${var.vm_db_name}-nic"
  location            = var.location
  resource_group_name = var.resource_group

  ip_configuration {
    name                          = "internal"
    subnet_id                     = var.subnet_db_id
    private_ip_address_allocation = "Static"
    private_ip_address            = "10.0.2.5"
  }
}

# NIC Backup (p√∫blica + privada)
resource "azurerm_network_interface" "bkp" {
  name                = "${var.vm_bkp_name}-nic"
  location            = var.location
  resource_group_name = var.resource_group

  ip_configuration {
    name                          = "internal"
    subnet_id                     = var.subnet_db_id
    private_ip_address_allocation = "Dynamic"
    public_ip_address_id          = azurerm_public_ip.bkp.id
  }
}

# Asociaciones NSG
resource "azurerm_network_interface_security_group_association" "db_nsg_assoc" {
  network_interface_id      = azurerm_network_interface.db.id
  network_security_group_id = azurerm_network_security_group.db_nsg.id
}

resource "azurerm_network_interface_security_group_association" "bkp_nsg_assoc" {
  network_interface_id      = azurerm_network_interface.bkp.id
  network_security_group_id = azurerm_network_security_group.bkp_nsg.id
}

# VM Database
resource "azurerm_linux_virtual_machine" "db" {
  name                   = var.vm_db_name
  location               = var.location
  resource_group_name    = var.resource_group
  size                   = "Standard_B2s"
  admin_username         = var.admin_user
  network_interface_ids  = [azurerm_network_interface.db.id]

  os_disk {
    caching              = "ReadWrite"
    storage_account_type = "Standard_LRS"
  }

  source_image_reference {
    publisher = "Canonical"
    offer     = "0001-com-ubuntu-server-jammy"
    sku       = "22_04-lts-gen2"
    version   = "latest"
  }

  admin_ssh_key {
    username   = var.admin_user
    public_key = var.ssh_public_key
  }

  depends_on = [azurerm_network_interface_security_group_association.db_nsg_assoc]
}

# VM Backup
resource "azurerm_linux_virtual_machine" "bkp" {
  name                   = var.vm_bkp_name
  location               = var.location
  resource_group_name    = var.resource_group
  size                   = "Standard_B2s"
  admin_username         = var.admin_user
  network_interface_ids  = [azurerm_network_interface.bkp.id]

  os_disk {
    caching              = "ReadWrite"
    storage_account_type = "Standard_LRS"
  }

  source_image_reference {
    publisher = "Canonical"
    offer     = "0001-com-ubuntu-server-jammy"
    sku       = "22_04-lts-gen2"
    version   = "latest"
  }

  admin_ssh_key {
    username   = var.admin_user
    public_key = var.ssh_public_key
  }

  depends_on = [azurerm_network_interface_security_group_association.bkp_nsg_assoc]
}

# Generar archivo de configuraci√≥n SSH
resource "local_file" "ssh_config" {
  content = <<EOT
Host vm-bkp
  HostName ${azurerm_public_ip.bkp.ip_address}
  User ${var.admin_user}
  IdentityFile ~/.ssh/id_rsa

Host vm-db
  HostName ${azurerm_linux_virtual_machine.db.private_ip_address}
  User ${var.admin_user}
  ProxyJump vm-bkp
  IdentityFile ~/.ssh/id_rsa
EOT

  filename = "/home/${var.admin_user}/.ssh/config"
}
```
> Al final del archivo de Terraform, este bloque genera autom√°ticamente un archivo SSH en tu m√°quina, sobrescribiendo cualquier configuraci√≥n previa. Gracias a ello, con un solo comando puedes conectarte a la VM de backup y a la base de datos sin necesidad de configurar nada manualmente.


---

#### `modules/compute/variables.tf` 

```hcl
variable "location"       { type = string }
variable "resource_group" { type = string }
variable "subnet_db_id"   { type = string }
variable "vm_db_name"     { type = string }
variable "vm_bkp_name"    { type = string }
variable "admin_user"     { type = string }
variable "ssh_public_key" { type = string }
```

---

#### `modules/compute/outputs.tf` 

```hcl
output "vm_db_private_ip"  { value = azurerm_network_interface.db.private_ip_address }
output "vm_bkp_private_ip" { value = azurerm_network_interface.bkp.private_ip_address }
output "vm_bkp_public_ip"  { value = azurerm_public_ip.bkp.ip_address }
output "db_nsg_id"         { value = azurerm_network_security_group.db_nsg.id }
output "bkp_nsg_id"        { value = azurerm_network_security_group.bkp_nsg.id }
```

---

### üîó 3.4.4 M√≥dulo peering

#### `modules/peering/main.tf` 

```hcl
resource "azurerm_virtual_network_peering" "compute_to_aks" {
  name                      = "peer-compute-to-aks"
  resource_group_name       = var.resource_group
  virtual_network_name      = basename(var.vnet_compute_id)
  remote_virtual_network_id = var.vnet_aks_id
  allow_forwarded_traffic   = true
  allow_virtual_network_access = true
}

resource "azurerm_virtual_network_peering" "aks_to_compute" {
  name                      = "peer-aks-to-compute"
  resource_group_name       = var.resource_group
  virtual_network_name      = basename(var.vnet_aks_id)
  remote_virtual_network_id = var.vnet_compute_id
  allow_forwarded_traffic   = true
  allow_virtual_network_access = true
}
```

---

#### `modules/peering/variables.tf` 

```hcl
variable "resource_group"  { type = string }
variable "vnet_compute_id" { type = string }
variable "vnet_aks_id"     { type = string }
```

---

#### `modules/peering/outputs.tf` 

```hcl
output "peering_compute_to_aks_id" { value = azurerm_virtual_network_peering.compute_to_aks.id }
output "peering_aks_to_compute_id" { value = azurerm_virtual_network_peering.aks_to_compute.id }
```

---

### ‚ò∏Ô∏è 3.4.5 M√≥dulo aks

La parte del cl√∫ster AKS se encuentra comentada por el momento debido a las limitaciones de la suscripci√≥n gratuita de Azure, que impiden desplegar este recurso. Como se explic√≥ anteriormente, para poder seguir avanzando en el proyecto se est√°n levantando √∫nicamente las m√°quinas virtuales, mientras que el cl√∫ster se probar√° de forma local con Minikube o en el futuro con una suscripci√≥n de pago.

#### `modules/aks/main.tf` 

```hcl
resource "azurerm_kubernetes_cluster" "aks" {
  name                = var.cluster_name
  location            = var.location
  resource_group_name = var.resource_group
  dns_prefix          = "${var.cluster_name}-dns"

  default_node_pool {
    name                = "default"
    node_count          = var.node_count
    vm_size             = var.node_vm_size
    vnet_subnet_id      = var.subnet_aks_id
    type                = "VirtualMachineScaleSets"
    enable_auto_scaling = false
  }

  identity {
    type = "SystemAssigned"
  }

  network_profile {
    network_plugin    = "azure"
    load_balancer_sku = "standard"
    outbound_type     = "loadBalancer"
  }
}
```

---

#### `modules/aks/variables.tf` 

```hcl
variable "location"        { type = string }
variable "resource_group"  { type = string }

variable "cluster_name"    { type = string }
variable "node_count"      { type = number }
variable "node_vm_size"    { type = string }
variable "subnet_aks_id"   { type = string }
```

---

#### `modules/aks/outputs.tf` 

```hcl
output "cluster_name" {
    value = azurerm_kubernetes_cluster.aks.name
}

output "kube_config" {
    value     = azurerm_kubernetes_cluster.aks.kube_config_raw
    sensitive = true
}
```

[üîù Volver a la tabla de contenidos üîù](#-tabla-de-contenidos) 

---

## üöÄ 3.5 Desplegar la infraestructura

En esta secci√≥n se muestra c√≥mo levantar la infraestructura explicada anteriormente utilizando Terraform de manera sencilla. Se detallan los pasos b√°sicos que permiten preparar el entorno, revisar los cambios previstos, desplegar los recursos y, cuando ya no son necesarios, eliminarlos de forma segura para evitar costes innecesarios.

Los comandos principales que hay que seguir para desplegar esta infraestructura son los siguinetes:

```bash
terraform init
```

> Prepara el directorio de trabajo, descarga los proveedores necesarios y configura el backend remoto.

```bash
terraform plan
```

> Muestra un resumen de los cambios que se aplicar√°n sin modificar nada todav√≠a, permitiendo verificar que todo est√° correcto.

```bash
terraform apply
```

> Aplica los cambios planificados, creando realmente la infraestructura. Durante su ejecuci√≥n se muestran los outputs definidos, como las IPs de las VMs, para poder acceder a ellas.

```bash
terraform destroy
```

> Elimina todos los recursos desplegados (VMs, redes, peering, etc.), liberando los servicios de Azure y reduciendo costes cuando ya no se necesitan.

Cuando ejecutamos el terraform apply podemos ver como se crean todos los recursos:

![](imgs/03/2.png)

[üîù Volver a la tabla de contenidos üîù](#-tabla-de-contenidos) 














